---
title: "final_project"
author: "HU Wenxiao"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(lubridate)
library(tidytext)
library(SnowballC)
library(ggwordcloud)
```

# Policy data

## 1. Data wrangling

### 1.1 data cleaning

#selecting data

```{r}
d <-read_csv("data/region-g20+countries.csv")
```

```{r}
d_policy <- d |>
  select('Country', 'Name of policy','Energy Type','Date of announcement','Date of entry info force','Value committed, USD')
```

#data integrity check for policy date NA

```{r}
d_policy |>
  mutate(Date_missing=as.numeric(is.na(`Date of entry info force`)),.after=`Date of entry info force`)|>
  group_by(Country)|>
  summarize(N_Date_missing=sum(Date_missing))
```

#data integrity check for value committed "0"\--NA

```{r}
d_policy$`Value committed, USD`[d_policy$`Value committed, USD`==0]<-NA
```

```{r}
d_policy |>
  mutate(Value_missing=as.numeric(is.na(`Value committed, USD`)),.after=`Value committed, USD`)|>
  group_by(Country)|>
  summarize(N_Value_missing=sum(Value_missing))
```

#deletion of NA

```{r}
d_policy <- d_policy[complete.cases(d_policy),]
```

```{r}
write.csv(d_policy,"data/clean_policy_dataset.csv",row.names = FALSE)
```

# 2. Text mining

## 2.1 Wrangling 

```{r}
d_policy_tokenized=d_policy |>
  select(Country,`Name of policy`,'Date of entry info force')|>
  unnest_tokens(word,`Name of policy`)
d_policy_tokenized <- d_policy_tokenized |>
  filter(!str_detect(word, "[0-9]+"))
```

remove stop words

```{r}
data("stop_words")
d_policy_tokenized=d_policy_tokenized |>
  anti_join(stop_words,by="word")
```

create stem

```{r}
d_policy_tokenized=d_policy_tokenized |>
  mutate(stem=wordStem(word))
```

## 2.2 Data analysis

### word frequencies

```{r}
word_frequency = d_policy_tokenized |>
  count(stem, sort = TRUE)
```

### plot a word cloud

```{r}
word_frequency |>
  slice(1:100)|>
  ggplot(aes(label=stem,size=n))+
  scale_size_area(max_size=8)+
  geom_text_wordcloud()+
  theme_minimal()
```

### 2.3 calculate country-level word frequencies

create a df with only stem from their own countries' policy

```{r}
d_policy_frequencies <- d_policy_tokenized |>
  group_by(Country, stem) |>
  count()|>
  mutate(prop=n/sum(word_frequency$n)*100)
```

### 2.4 create document-term matrix

A. use pivot wider

```{r}
new <-d_policy_frequencies |>
  pivot_wider(names_from=stem,values_from=prop,values_fill=0)|>
  select(-n)|>
  group_by(Country)|>
  summarise(across(everything(),~if(all(.==0))0 else sum(.)))
```

B. document-term matrix

create a df with all stem

```{r}
all_combinations<-expand.grid(Country=unique(d_policy_frequencies$Country),stem=unique(word_frequency$stem))
```

```{r}
d_policy_merged <-left_join(all_combinations,d_policy_frequencies,by=c("Country","stem"))
```

```{r}
d_policy_merged <-d_policy_merged |>
  mutate(n=ifelse(is.na(n),0,n))|>
  mutate(prop=ifelse(is.na(prop),0,prop))|>
  arrange(Country)
```

make dtm

```{r}
dtm=d_policy_merged |> cast_dtm(Country,stem,prop)
```
