---
title: "clean_country_policy"
author: "Athena Zhang"
date: "2023-12-05"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# loading package
library(tidyverse)
library(lubridate)
library(tidytext)
library(SnowballC)
library(ggwordcloud)
library(wordcloud2)
```

# 1. Data cleaning for policy and index

```{r}
# importing data
d <- read.csv('data/ALL_COUNTRY.csv')
```

```{r}
# selecting relevant data
policy <- d |> 
  select('Country', 'Policy.Category', 'Name.of.policy', 'Energy.Type', 'Date.of.entry.info.force','Value.committed..USD')

```

```{r}
# removing NA for date of entry
policy$'Date.of.entry.info.force' <- na_if(policy$'Date.of.entry.info.force', "")
policy <- policy |>
  filter(!is.na(Date.of.entry.info.force))
```

```{r}
# remove 0 for value committed
policy <- policy |>
  filter(Value.committed..USD != 0)
```

```{r}
# keep 'clean conditional' and 'clean unconditional' policies
policy <- policy |>
  filter(Policy.Category %in% c('clean conditional', 'clean unconditional'))
```

```{r}
# importing data
epi <- read_csv('data/epi2022regionalresults05302022.csv')
```

```{r}
# selecting only country and climate change relevant measures
cc <- epi |>
  select(country, PCC.new)
```

```{r}
setdiff(policy$Country, cc$country) #in policy but not in cc
# recode countries in cc
cc <- cc |> mutate(country = recode(country,
                           "Netherlands" = "The Netherlands",
                           "United States of America" = "United States",
                           "Viet Nam" = "Vietnam",
                           "South Korea" = "Republic of Korea"))
```

```{r}
# remove 'European Institutions' from country in policy
policy <- policy |>
  filter(Country != 'European Institutions')
```

```{r}
#rename for 'country'
cc <- rename(cc, Country = country)
```

## 2. Text mining for policy data

## 2.1 data wrangling

```{r}
policy_tokenized=policy |>
  select(Country,`Name.of.policy`,'Date.of.entry.info.force')|>
  unnest_tokens(word,`Name.of.policy`)
policy_tokenized <- policy_tokenized |>
  filter(!str_detect(word, "[0-9]+"))
```

```{r}
# remove stop words
data("stop_words")
policy_tokenized=policy_tokenized |>
  anti_join(stop_words,by="word")
```

```{r}
#create stem
policy_tokenized=policy_tokenized |>
  mutate(stem=wordStem(word))
```

## 2.2 data analysis

### A. word frequency

```{r}
word_frequency = policy_tokenized |>
  count(stem, sort = TRUE)
```

### B. plot a word cloud

```{r}
word_frequency |>
  slice(1:100)|>
  ggplot(aes(label=stem,size=n))+
  scale_size_area(max_size=8)+
  geom_text_wordcloud()+
  theme_minimal()
```

```{r}
wordcloud2(word_frequency,color="darkgreen")
```

### C. compare policy design between 2020,2021,2022

```{r}

```

### D. calculate country-level word frequencies

```{r}
#calculate the proprtion for each keywords
policy_frequencies <- policy_tokenized |>
  group_by(Country, stem) |>
  count()|>
  mutate(prop=n/sum(word_frequency$n)*100)
```

```{r}
# creating pivoted dataframe to perform linear regression
d_policy_pivoted <- policy_frequencies |>
  pivot_wider(
    id_cols = Country,
    names_from = stem,
    values_from = prop
  )
```

```{r}
# turn NA values to 0
d_policy_pivoted[is.na(d_policy_pivoted)] <- 0
```

```{r}
#joining policy data and climate change index data
policy_cc <- left_join(d_policy_pivoted, cc, by = "Country")
```

```{r}
#relocating cc index column
policy_cc <- policy_cc |> relocate(PCC.new, .after = Country)
```

# 3. LASSO regression

```{r}
# set predictors and outcome
y <- policy_cc$PCC.new

x <- policy_cc |> select(agricultur:yorker) |> as.matrix()
```

```{r}
# load LASSO regression package
library(glmnet)
```

```{r}
model <- glmnet(x, y , lambda = 2, family = "gaussian", intercept = TRUE, alpha = 1)

summary(model)

lasso_coef <- model$beta |> as.matrix() |> as.data.frame() |> 
  rownames_to_column(var = "predictor") |> as_tibble()

lasso_coef_nonzero <- lasso_coef |> filter(s0 != 0)

lasso_coef_nonzero |> 
  ggplot() + geom_bar(aes(y = predictor, x = s0), stat = "identity")

```
